import streamlit as st
import requests
import json
import time
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®
API_BASE_URL = os.getenv("DEFAULT_API_BASE_URL", "https://api.siliconflow.cn")
# ç§»é™¤å¯¹Expressåç«¯çš„ä¾èµ–ï¼Œä¸å†éœ€è¦è¿™äº›é…ç½®
# DEFAULT_PORT = os.getenv("PORT", "3000")
# BACKEND_URL = f"http://localhost:{DEFAULT_PORT}/api"

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="å¢å¼ºå‹è‡ªç›‘ç£æç¤ºä¼˜åŒ–ç³»ç»Ÿ",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSS
st.markdown("""
<style>
    /* ç°ä»£åŒ–è®¾è®¡ */
    .main-header {
        font-size: 2.5rem;
        margin-bottom: 1rem;
        font-weight: 600;
        color: #1E3A8A;
    }
    .sub-header {
        font-size: 1.5rem;
        margin-bottom: 1rem;
        font-weight: 500;
        color: #2563EB;
    }
    
    /* å¡ç‰‡å¼è®¾è®¡ */
    .card {
        background-color: #FFFFFF;
        border-radius: 0.75rem;
        padding: 1.5rem;
        margin-bottom: 1.5rem;
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
        transition: all 0.3s ease;
    }
    .card:hover {
        box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
    }
    
    /* æ›´å¥½çš„è¾“å‡ºå®¹å™¨ */
    .output-container {
        background-color: #F9FAFB;
        border-radius: 0.75rem;
        padding: 1.2rem;
        margin-bottom: 1rem;
        border-left: 4px solid #E5E7EB;
        font-family: 'Roboto Mono', monospace;
    }
    
    /* è¯„ä¼°ç»“æœé¢œè‰² */
    .better {
        border-left: 4px solid #10B981;
        background-color: #ECFDF5;
    }
    .worse {
        border-left: 4px solid #EF4444;
        background-color: #FEF2F2;
    }
    .similar {
        border-left: 4px solid #F59E0B;
        background-color: #FFFBEB;
    }
    
    /* å†å²è®°å½•é¡¹ */
    .history-item {
        padding: 0.75rem;
        margin-bottom: 0.75rem;
        border-radius: 0.5rem;
        background-color: #F9FAFB;
        transition: all 0.2s ease;
    }
    .history-item:hover {
        background-color: #F3F4F6;
    }
    .history-item.better {
        border-left: 4px solid #10B981;
    }
    .history-item.not-better {
        border-left: 4px solid #EF4444;
    }
    
    /* æç¤ºå’Œå¼•å¯¼ */
    .guide-box {
        background-color: #EFF6FF;
        border: 1px solid #DBEAFE;
        border-radius: 0.5rem;
        padding: 1rem;
        margin-bottom: 1.2rem;
        color: #1E40AF;
    }
    .tip-box {
        background-color: #ECFDF5;
        border: 1px solid #D1FAE5;
        border-radius: 0.5rem;
        padding: 1rem;
        margin-bottom: 1.2rem;
        color: #065F46;
    }
    
    /* æ›´å¥½çš„æŒ‰é’® */
    .stButton>button {
        border-radius: 0.5rem !important;
        font-weight: 500 !important;
        transition: all 0.2s !important;
    }
    
    /* æµå¼è¾“å‡ºåŒºåŸŸ */
    .stream-output {
        background-color: #F8FAFC;
        border-radius: 0.5rem;
        padding: 1rem;
        margin-top: 0.5rem;
        border-left: 3px solid #3B82F6;
        min-height: 100px;
        font-family: 'Roboto Mono', monospace;
        white-space: pre-wrap;
        animation: pulse 2s infinite;
    }
    @keyframes pulse {
        0% {
            border-color: #3B82F6;
        }
        50% {
            border-color: #60A5FA;
        }
        100% {
            border-color: #3B82F6;
        }
    }
    
    /* æ­¥éª¤æŒ‡ç¤ºå™¨ */
    .step-container {
        display: flex;
        justify-content: space-between;
        margin-bottom: 2rem;
        position: relative;
    }
    .step-container::before {
        content: "";
        position: absolute;
        top: 15px;
        left: 0;
        right: 0;
        height: 2px;
        background-color: #E5E7EB;
        z-index: 1;
    }
    .step {
        background-color: #FFFFFF;
        border: 2px solid #E5E7EB;
        border-radius: 50%;
        width: 30px;
        height: 30px;
        display: flex;
        align-items: center;
        justify-content: center;
        font-weight: bold;
        color: #6B7280;
        position: relative;
        z-index: 2;
    }
    .step.active {
        background-color: #3B82F6;
        border-color: #3B82F6;
        color: white;
    }
    .step.completed {
        background-color: #10B981;
        border-color: #10B981;
        color: white;
    }
    .step-label {
        position: absolute;
        top: 35px;
        font-size: 0.8rem;
        color: #6B7280;
        text-align: center;
        width: 80px;
        left: -25px;
    }
</style>
""", unsafe_allow_html=True)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'initialized' not in st.session_state:
    st.session_state.initialized = False
    st.session_state.api_configured = False
    st.session_state.current_view = "config"
    st.session_state.current_iteration = 0
    st.session_state.max_iterations = 10
    st.session_state.samples = []
    st.session_state.current_best_prompt = ""
    st.session_state.current_best_outputs = {}
    st.session_state.new_prompt = ""
    st.session_state.new_outputs = {}
    st.session_state.evaluations = {}
    st.session_state.analysis = ""
    st.session_state.optimization_history = []
    st.session_state.is_optimizing = False
    st.session_state.available_models = []

# æ–°å¢LLMæœåŠ¡ç±»ï¼Œç”¨äºæ›¿ä»£Expressåç«¯
class LLMService:
    def __init__(self, api_key=None, base_url=None):
        self.api_key = api_key
        self.base_url = base_url or os.getenv("DEFAULT_API_BASE_URL", "https://api.siliconflow.cn")
    
    def get_headers(self):
        headers = {"Content-Type": "application/json"}
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        return headers
    
    def call_llm_api(self, endpoint, data):
        try:
            response = requests.post(
                f"{self.base_url}/{endpoint}", 
                json=data, 
                headers=self.get_headers()
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            raise Exception(f"LLM APIè°ƒç”¨å¤±è´¥: {str(e)}")
    
    def call_llm_api_stream(self, endpoint, data):
        """æµå¼è°ƒç”¨LLM API"""
        try:
            # ç¡®ä¿è®¾ç½®streamä¸ºTrue
            data["stream"] = True
            
            response = requests.post(
                f"{self.base_url}/{endpoint}", 
                json=data, 
                headers=self.get_headers(),
                stream=True  # è®¾ç½®requestsä¸ºæµå¼è¯·æ±‚
            )
            response.raise_for_status()
            return response
        except Exception as e:
            raise Exception(f"æµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}")
    
    def generate_samples(self, task_description):
        # å®ç°æ ·æœ¬ç”Ÿæˆ
        prompt = f"""è¯·ä¸ºä»¥ä¸‹ä»»åŠ¡ç”Ÿæˆ5ä¸ªæµ‹è¯•æ ·ä¾‹ï¼Œæ¯ä¸ªæ ·ä¾‹åº”åŒ…å«é—®é¢˜å’ŒæœŸæœ›çš„å›ç­”æ ‡å‡†ï¼š
        
ä»»åŠ¡æè¿°ï¼š{task_description}

è¯·è¿”å›JSONæ ¼å¼ï¼š
[
    {{"question": "é—®é¢˜1", "expected": "æœŸæœ›æ ‡å‡†1"}},
    ...
]
"""
        response = self.call_llm_api("v1/chat/completions", {
            "model": "deepseek-ai/DeepSeek-V3",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.7
        })
        
        try:
            content = response.get("choices", [{}])[0].get("message", {}).get("content", "")
            # æå–JSONéƒ¨åˆ†
            json_str = content[content.find("["):content.rfind("]")+1]
            samples = json.loads(json_str)
            # ç»™æ¯ä¸ªæ ·æœ¬æ·»åŠ ID
            for i, sample in enumerate(samples):
                sample['id'] = i + 1
            return samples
        except:
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ç®€åŒ–ç‰ˆæ ·æœ¬
            return [
                {"question": "ç¤ºä¾‹é—®é¢˜1", "expected": "æœŸæœ›å›ç­”æ ‡å‡†1"},
                {"question": "ç¤ºä¾‹é—®é¢˜2", "expected": "æœŸæœ›å›ç­”æ ‡å‡†2"}
            ]
    
    def execute_prompt(self, prompt, question):
        """æ‰§è¡Œæç¤ºè¯è·å–è¾“å‡º"""
        full_prompt = f"{prompt}\n\n{question}"
        response = self.call_llm_api("v1/chat/completions", {
            "model": "deepseek-ai/DeepSeek-V3",
            "messages": [{"role": "user", "content": full_prompt}],
            "temperature": 0.3
        })
        return response.get("choices", [{}])[0].get("message", {}).get("content", "")
    
    def execute_prompt_stream(self, prompt, question, callback):
        """å¸¦æµå¼è¾“å‡ºçš„æ‰§è¡Œæç¤ºè¯"""
        full_prompt = f"{prompt}\n\n{question}"
        response = self.call_llm_api_stream("v1/chat/completions", {
            "model": "deepseek-ai/DeepSeek-V3",
            "messages": [{"role": "user", "content": full_prompt}],
            "temperature": 0.3
        })
        
        # ç´¯ç§¯å®Œæ•´å“åº”
        full_response = ""
        
        # å¤„ç†æµå¼å“åº”
        for line in response.iter_lines():
            if line:
                line = line.decode('utf-8')
                if line.startswith('data: '):
                    data = line[6:]  # ç§»é™¤ "data: " å‰ç¼€
                    if data == "[DONE]":
                        break
                    
                    try:
                        json_data = json.loads(data)
                        delta = json_data.get("choices", [{}])[0].get("delta", {}).get("content", "")
                        if delta:
                            full_response += delta
                            callback(delta, full_response)  # å›è°ƒå‡½æ•°å¤„ç†æ¯ä¸ªå¢é‡
                    except json.JSONDecodeError:
                        continue
        
        return full_response
    
    def optimize_prompt(self, current_prompt, current_output, task_description, history=""):
        """ä¼˜åŒ–æç¤ºè¯"""
        prompt = f"""è¯·ä¼˜åŒ–ä»¥ä¸‹æç¤ºè¯ï¼Œä½¿å…¶æ›´æœ‰æ•ˆåœ°å®Œæˆä»»åŠ¡ï¼š

ä»»åŠ¡æè¿°ï¼š{task_description}

å½“å‰æç¤ºè¯ï¼š
{current_prompt}

å½“å‰è¾“å‡ºç¤ºä¾‹ï¼š
{current_output}

ä¼˜åŒ–å†å²ï¼š
{history}

è¯·ç›´æ¥è¿”å›ä¼˜åŒ–åçš„å®Œæ•´æç¤ºè¯ï¼Œä¸è¦æœ‰å…¶ä»–è§£é‡Šã€‚"""
        
        response = self.call_llm_api("v1/chat/completions", {
            "model": "deepseek-ai/DeepSeek-V3",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.7
        })
        return response.get("choices", [{}])[0].get("message", {}).get("content", "")
    
    def evaluate_outputs(self, output_a, output_b, task_description, question):
        """è¯„ä¼°è¾“å‡ºè´¨é‡"""
        prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹ä¸¤ä¸ªè¾“å‡ºï¼Œå“ªä¸€ä¸ªæ›´å¥½åœ°å®Œæˆäº†ä»»åŠ¡ï¼š

ä»»åŠ¡æè¿°ï¼š{task_description}

é—®é¢˜ï¼š{question}

è¾“å‡ºAï¼š
{output_a}

è¾“å‡ºBï¼š
{output_b}

è¯·ç»™å‡ºè¯¦ç»†è¯„ä¼°ï¼Œå¹¶æ˜ç¡®æŒ‡å‡ºå“ªä¸ªæ›´å¥½ï¼ˆAã€Bæˆ–ç›¸ä¼¼ï¼‰ï¼š"""
        
        response = self.call_llm_api("v1/chat/completions", {
            "model": "deepseek-ai/DeepSeek-V3",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3
        })
        result = response.get("choices", [{}])[0].get("message", {}).get("content", "")
        
        # ç®€å•è§£æç»“æœ
        if "è¾“å‡ºAæ›´å¥½" in result or "Aæ›´å¥½" in result:
            winner = "A"
        elif "è¾“å‡ºBæ›´å¥½" in result or "Bæ›´å¥½" in result:
            winner = "B"
        else:
            winner = "similar"
            
        return {
            "details": result,
            "winner": winner
        }
    
    def analyze_changes(self, old_prompt, new_prompt, task_description):
        """åˆ†ææç¤ºè¯å˜åŒ–"""
        prompt = f"""è¯·åˆ†æä»¥ä¸‹æç¤ºè¯çš„å˜åŒ–ï¼Œå¹¶è§£é‡Šè¿™äº›å˜åŒ–å¦‚ä½•æå‡äº†æç¤ºè¯çš„æ•ˆæœï¼š

ä»»åŠ¡æè¿°ï¼š{task_description}

æ—§æç¤ºè¯ï¼š
{old_prompt}

æ–°æç¤ºè¯ï¼š
{new_prompt}

è¯·è¯¦ç»†è¯´æ˜ï¼š"""
        
        response = self.call_llm_api("v1/chat/completions", {
            "model": "deepseek-ai/DeepSeek-V3",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.5
        })
        return response.get("choices", [{}])[0].get("message", {}).get("content", "")

# è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
def get_available_models():
    # ä»…è¿”å›DeepSeekç³»åˆ—æ¨¡å‹
    return [
        "deepseek-ai/DeepSeek-V3",
        "deepseek-ai/DeepSeek-R1"
    ]

# è°ƒç”¨APIå‡½æ•°å·²ä¸å†éœ€è¦ï¼Œå› ä¸ºä½¿ç”¨LLMServiceç±»ç›´æ¥è°ƒç”¨

# é…ç½®API
def configure_api(api_key, base_url, models):
    try:
        # åˆ›å»ºLLMServiceå®ä¾‹å¹¶ä¿å­˜åœ¨ä¼šè¯çŠ¶æ€ä¸­
        st.session_state.llm_service = LLMService(api_key=api_key, base_url=base_url)
        # ä¿å­˜APIé…ç½®ä¿¡æ¯
        st.session_state.api_key = api_key
        st.session_state.base_url = base_url
        st.session_state.api_configured = True
        return True
    except Exception as e:
        st.error(f"APIé…ç½®å¤±è´¥: {str(e)}")
        return False

# ç”Ÿæˆæµ‹è¯•æ ·æœ¬
def generate_samples(task_description):
    try:
        # ç›´æ¥ä½¿ç”¨LLMServiceå®ä¾‹
        if "llm_service" in st.session_state:
            samples = st.session_state.llm_service.generate_samples(task_description)
            # ç»™æ¯ä¸ªæ ·æœ¬æ·»åŠ ID
            for i, sample in enumerate(samples):
                sample['id'] = i + 1
            return samples
        else:
            st.error("è¯·å…ˆé…ç½®API")
            return []
    except Exception as e:
        st.error(f"ç”Ÿæˆæ ·æœ¬å¤±è´¥: {str(e)}")
        return []

# æ‰§è¡Œæç¤ºè¯
def execute_prompt(prompt, question, use_stream=False):
    try:
        # ç›´æ¥ä½¿ç”¨LLMServiceå®ä¾‹
        if "llm_service" in st.session_state:
            if use_stream:
                # åˆ›å»ºä¸€ä¸ªç©ºçš„å ä½ç¬¦ç”¨äºæ˜¾ç¤ºæµå¼è¾“å‡º
                output_placeholder = st.empty()
                
                # å®šä¹‰å›è°ƒå‡½æ•°å¤„ç†æµå¼è¾“å‡º
                def stream_callback(delta, full_response):
                    output_placeholder.markdown(full_response)
                
                # è°ƒç”¨æµå¼API
                return st.session_state.llm_service.execute_prompt_stream(
                    prompt, question, stream_callback
                )
            else:
                # ä½¿ç”¨åŸæœ‰çš„éæµå¼API
                return st.session_state.llm_service.execute_prompt(prompt, question)
        else:
            st.error("è¯·å…ˆé…ç½®API")
            return ""
    except Exception as e:
        st.error(f"æ‰§è¡Œæç¤ºè¯å¤±è´¥: {str(e)}")
        return ""

# ä¼˜åŒ–æç¤ºè¯
def optimize_prompt(current_prompt, current_output, task_description, history=""):
    try:
        # ç›´æ¥ä½¿ç”¨LLMServiceå®ä¾‹
        if "llm_service" in st.session_state:
            return st.session_state.llm_service.optimize_prompt(
                current_prompt, 
                current_output, 
                task_description, 
                history
            )
        else:
            st.error("è¯·å…ˆé…ç½®API")
            return ""
    except Exception as e:
        st.error(f"ä¼˜åŒ–æç¤ºè¯å¤±è´¥: {str(e)}")
        return ""

# è¯„ä¼°è¾“å‡º
def evaluate_outputs(output_a, output_b, task_description, question):
    try:
        # ç›´æ¥ä½¿ç”¨LLMServiceå®ä¾‹
        if "llm_service" in st.session_state:
            result = st.session_state.llm_service.evaluate_outputs(
                output_a, 
                output_b, 
                task_description, 
                question
            )
            # æ ¹æ®winneråˆ¤æ–­è¿”å›çš„ç»“æœ
            if result['winner'] == 'A':
                return "Aæ›´å¥½"
            elif result['winner'] == 'B':
                return "Bæ›´å¥½"
            else:
                return "ç›¸ä¼¼"
        else:
            st.error("è¯·å…ˆé…ç½®API")
            return "ç›¸ä¼¼"
    except Exception as e:
        st.error(f"è¯„ä¼°è¾“å‡ºå¤±è´¥: {str(e)}")
        return "ç›¸ä¼¼"

# åˆ†ææç¤ºå˜åŒ–
def analyze_changes(old_prompt, new_prompt, task_description):
    try:
        # ç›´æ¥ä½¿ç”¨LLMServiceå®ä¾‹
        if "llm_service" in st.session_state:
            return st.session_state.llm_service.analyze_changes(
                old_prompt, 
                new_prompt, 
                task_description
            )
        else:
            st.error("è¯·å…ˆé…ç½®API")
            return ""
    except Exception as e:
        st.error(f"åˆ†ææç¤ºå˜åŒ–å¤±è´¥: {str(e)}")
        return ""

# æ‰§è¡Œå½“å‰æœ€ä½³æç¤ºè¯
def run_current_best_prompt():
    outputs = {}
    
    with st.spinner("æ­£åœ¨æ‰§è¡Œå½“å‰æç¤ºè¯..."):
        for sample in st.session_state.samples:
            output = execute_prompt(st.session_state.current_best_prompt, sample['question'])
            outputs[sample['id']] = output
    
    st.session_state.current_best_outputs = outputs
    return outputs

# è·å–ä¼˜åŒ–å†å²æ‘˜è¦
def get_optimization_history_summary():
    if not st.session_state.optimization_history:
        return ""
    
    # åªå–æœ€è¿‘3æ¬¡è¿­ä»£çš„å†å²
    recent_history = st.session_state.optimization_history[-3:]
    
    return "\n".join([
        f"è¿­ä»£{item['iteration']}: {'æ”¹è¿›æˆåŠŸ' if item['is_better'] else 'æœªæ”¹è¿›'}"
        for item in recent_history
    ])

# åˆ¤æ–­æ˜¯å¦åº”è¯¥æ›´æ–°æœ€ä½³æç¤º
def should_update_best_prompt(evaluations):
    better_count = 0
    worse_count = 0
    
    for sample_id, result in evaluations.items():
        if result == "Bæ›´å¥½":
            better_count += 1
        elif result == "Aæ›´å¥½":
            worse_count += 1
    
    # å¦‚æœæœ‰æ›´å¤šæ ·æœ¬è®¤ä¸ºæ–°æç¤ºæ›´å¥½ï¼Œåˆ™æ›´æ–°
    return better_count > worse_count

# è¿è¡Œä¼˜åŒ–æ­¥éª¤
def run_optimization_step():
    if st.session_state.current_iteration >= st.session_state.max_iterations:
        st.session_state.is_optimizing = False
        st.session_state.current_view = "results"
        return
    
    st.session_state.current_iteration += 1
    
    # 1. ç”Ÿæˆæ–°æç¤ºå€™é€‰
    with st.spinner(f"æ­£åœ¨æ‰§è¡Œç¬¬ {st.session_state.current_iteration} æ¬¡ä¼˜åŒ–..."):
        new_prompt = optimize_prompt(
            st.session_state.current_best_prompt,
            json.dumps(st.session_state.current_best_outputs),
            st.session_state.task_description,
            get_optimization_history_summary()
        )
        
        if not new_prompt:
            st.error("ä¼˜åŒ–æç¤ºè¯å¤±è´¥")
            st.session_state.is_optimizing = False
            return
        
        st.session_state.new_prompt = new_prompt
        
        # 2. æ‰§è¡Œæ–°æç¤º
        new_outputs = {}
        for sample in st.session_state.samples:
            output = execute_prompt(new_prompt, sample['question'])
            new_outputs[sample['id']] = output
        
        st.session_state.new_outputs = new_outputs
        
        # 3. è¯„ä¼°æ–°æ—§è¾“å‡º
        evaluations = {}
        for sample in st.session_state.samples:
            sample_id = sample['id']
            output_a = st.session_state.current_best_outputs.get(sample_id, "")
            output_b = new_outputs.get(sample_id, "")
            
            evaluation = evaluate_outputs(
                output_a,
                output_b,
                st.session_state.task_description,
                sample['question']
            )
            
            evaluations[sample_id] = evaluation
        
        st.session_state.evaluations = evaluations
        
        # 4. åˆ†ææç¤ºå˜åŒ–
        analysis = analyze_changes(
            st.session_state.current_best_prompt,
            new_prompt,
            st.session_state.task_description
        )
        
        st.session_state.analysis = analysis
        
        # 5. æ ¹æ®è¯„ä¼°ç»“æœæ›´æ–°æœ€ä½³æç¤º
        is_better = should_update_best_prompt(evaluations)
        if is_better:
            st.session_state.current_best_prompt = new_prompt
            st.session_state.current_best_outputs = new_outputs
        
        # è®°å½•ä¼˜åŒ–å†å²
        st.session_state.optimization_history.append({
            "iteration": st.session_state.current_iteration,
            "prompt": new_prompt,
            "is_better": is_better,
            "analysis": analysis,
            "evaluations": evaluations
        })
    
    # å¦‚æœæ˜¯è‡ªåŠ¨æ¨¡å¼ï¼Œç»§ç»­ä¼˜åŒ–
    if st.session_state.auto_mode:
        time.sleep(1)  # çŸ­æš‚å»¶è¿Ÿï¼Œè®©UIæ›´æ–°
        run_optimization_step()

# é…ç½®è§†å›¾
def show_config_view():
    st.markdown("<h1 class='main-header'>SPO+ å¢å¼ºå‹è‡ªç›‘ç£æç¤ºä¼˜åŒ–ç³»ç»Ÿ</h1>", unsafe_allow_html=True)
    
    # æ­¥éª¤æŒ‡ç¤ºå™¨
    st.markdown("""
    <div class="step-container">
        <div class="step active">
            1
            <div class="step-label">APIé…ç½®</div>
        </div>
        <div class="step">
            2
            <div class="step-label">ä»»åŠ¡è®¾ç½®</div>
        </div>
        <div class="step">
            3
            <div class="step-label">ä¼˜åŒ–è¿‡ç¨‹</div>
        </div>
        <div class="step">
            4
            <div class="step-label">ä¼˜åŒ–ç»“æœ</div>
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    # å¼•å¯¼æç¤º
    st.markdown("""
    <div class="guide-box">
        <h4>ğŸš€ æ¬¢è¿ä½¿ç”¨æç¤ºè¯ä¼˜åŒ–ç³»ç»Ÿ</h4>
        <p>æœ¬ç³»ç»Ÿå¯ä»¥å¸®åŠ©æ‚¨è‡ªåŠ¨ä¼˜åŒ–AIæç¤ºè¯ï¼Œæé«˜AIè¾“å‡ºè´¨é‡ã€‚é¦–å…ˆï¼Œè¯·å®ŒæˆAPIé…ç½®å¹¶è®¾ç½®ä»»åŠ¡éœ€æ±‚ã€‚</p>
    </div>
    """, unsafe_allow_html=True)
    
    # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
    if not st.session_state.available_models:
        st.session_state.available_models = get_available_models()
    
    with st.form("config_form", clear_on_submit=False):
        st.markdown("<h2 class='sub-header'>APIè®¾ç½®</h2>", unsafe_allow_html=True)
        
        # APIè®¾ç½®å¡ç‰‡
        st.markdown('<div class="card">', unsafe_allow_html=True)
        api_key = st.text_input("API Key", type="password", 
                                help="è¾“å…¥æ‚¨çš„SiliconCloud APIå¯†é’¥")
        base_url = st.text_input("API Base URL", value=API_BASE_URL,
                                help="APIåŸºç¡€URLåœ°å€ï¼Œé»˜è®¤ä½¿ç”¨SiliconCloud")
        st.markdown('</div>', unsafe_allow_html=True)
        
        st.markdown("<h2 class='sub-header'>æ¨¡å‹è®¾ç½®</h2>", unsafe_allow_html=True)
        
        # æ¨¡å‹è®¾ç½®å¡ç‰‡
        st.markdown('<div class="card">', unsafe_allow_html=True)
        st.markdown("""
        <div class="tip-box">
            <p>ğŸ’¡ æç¤ºï¼šæ ¹æ®ä¸åŒä»»åŠ¡é€‰æ‹©é€‚åˆçš„æ¨¡å‹ã€‚DeepSeek-V3é€‚åˆéœ€è¦å¿«é€Ÿè¾“å‡ºçš„ä»»åŠ¡ï¼ŒDeepSeek-R1é€‚åˆæ•°å­¦æ¨ç†å’Œç²¾å‡†å›ç­”ã€‚</p>
        </div>
        """, unsafe_allow_html=True)
        
        col1, col2 = st.columns(2)
        
        with col1:
            optimizer_model = st.selectbox(
                "ä¼˜åŒ–æ¨¡å‹ (LLM-1)",
                options=st.session_state.available_models,
                index=0 if st.session_state.available_models else None,
                help="ç”¨äºç”Ÿæˆä¼˜åŒ–åçš„æç¤ºè¯"
            )
            
            evaluator_model = st.selectbox(
                "è¯„ä¼°æ¨¡å‹ (LLM-3)",
                options=st.session_state.available_models,
                index=0 if len(st.session_state.available_models) > 0 else None,
                help="ç”¨äºè¯„ä¼°ä¸åŒæç¤ºè¯çš„æ•ˆæœ"
            )
        
        with col2:
            executor_model = st.selectbox(
                "æ‰§è¡Œæ¨¡å‹ (LLM-2)",
                options=st.session_state.available_models,
                index=0 if len(st.session_state.available_models) > 0 else None,
                help="ç”¨äºæ‰§è¡Œæç¤ºè¯ç”Ÿæˆå›ç­”"
            )
            
            analyzer_model = st.selectbox(
                "åˆ†ææ¨¡å‹ (LLM-4)",
                options=st.session_state.available_models,
                index=0 if len(st.session_state.available_models) > 0 else None,
                help="ç”¨äºåˆ†ææç¤ºè¯çš„å˜åŒ–"
            )
        st.markdown('</div>', unsafe_allow_html=True)
        
        st.markdown("<h2 class='sub-header'>ä»»åŠ¡è®¾ç½®</h2>", unsafe_allow_html=True)
        
        # ä»»åŠ¡è®¾ç½®å¡ç‰‡
        st.markdown('<div class="card">', unsafe_allow_html=True)
        task_description = st.text_area(
            "ä»»åŠ¡éœ€æ±‚æè¿°", 
            height=100,
            placeholder="ä¾‹å¦‚ï¼šåˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿè§£é‡Šå¤æ‚ç§‘å­¦æ¦‚å¿µçš„AIåŠ©æ‰‹ï¼Œä½¿å…¶å›ç­”å‡†ç¡®ä¸”æ˜“äºç†è§£...",
            help="è¯¦ç»†æè¿°æ‚¨å¸Œæœ›AIå®Œæˆçš„ä»»åŠ¡å’ŒæœŸæœ›çš„è¾“å‡ºé£æ ¼"
        )
        
        initial_prompt = st.text_area(
            "åˆå§‹æç¤ºè¯", 
            height=150,
            placeholder="ä¾‹å¦‚ï¼šä½ æ˜¯ä¸€ä½æ“…é•¿è§£é‡Šå¤æ‚ç§‘å­¦æ¦‚å¿µçš„AIåŠ©æ‰‹ã€‚å½“ç”¨æˆ·æå‡ºç§‘å­¦é—®é¢˜æ—¶ï¼Œä½ åº”è¯¥...",
            help="è¾“å…¥æ‚¨ç›®å‰ä½¿ç”¨çš„æç¤ºè¯ä½œä¸ºä¼˜åŒ–èµ·ç‚¹"
        )
        
        st.markdown("""
        <div class="tip-box">
            <p>ğŸ’¡ æç¤ºï¼šåˆå§‹æç¤ºè¯è´¨é‡ä¼šå½±å“æœ€ç»ˆä¼˜åŒ–æ•ˆæœã€‚è¯·å°½é‡æä¾›è¯¦ç»†çš„ä»»åŠ¡æè¿°å’Œåˆå§‹æç¤ºè¯ã€‚</p>
        </div>
        """, unsafe_allow_html=True)
        
        col1, col2 = st.columns(2)
        
        with col1:
            max_iterations = st.number_input(
                "æœ€å¤§è¿­ä»£æ¬¡æ•°", 
                min_value=1, 
                max_value=20, 
                value=10,
                help="ç³»ç»Ÿå°†æ‰§è¡Œçš„æœ€å¤§ä¼˜åŒ–æ¬¡æ•°"
            )
        
        with col2:
            auto_mode = st.checkbox(
                "è‡ªåŠ¨æ¨¡å¼", 
                value=True,
                help="å¼€å¯åç³»ç»Ÿå°†è‡ªåŠ¨å®Œæˆå…¨éƒ¨ä¼˜åŒ–è¿‡ç¨‹ï¼Œæ— éœ€äººå·¥å¹²é¢„"
            )
            
            use_streaming = st.checkbox(
                "æµå¼è¾“å‡º", 
                value=True,
                help="å¼€å¯åå°†å®æ—¶æ˜¾ç¤ºAIç”Ÿæˆè¿‡ç¨‹"
            )
        st.markdown('</div>', unsafe_allow_html=True)
        
        col1, col2 = st.columns([3, 1])
        with col1:
            submitted = st.form_submit_button("å¼€å§‹ä¼˜åŒ–", use_container_width=True)
        with col2:
            show_example = st.form_submit_button("åŠ è½½ç¤ºä¾‹", use_container_width=True)
        
        if show_example:
            # åŠ è½½ç¤ºä¾‹ä»»åŠ¡å’Œæç¤ºè¯
            st.session_state.example_loaded = True
            st.rerun()
        
        if submitted:
            if not api_key:
                st.error("âš ï¸ è¯·è¾“å…¥API Key")
                return
            
            if not task_description:
                st.error("âš ï¸ è¯·è¾“å…¥ä»»åŠ¡éœ€æ±‚æè¿°")
                return
            
            if not initial_prompt:
                st.error("âš ï¸ è¯·è¾“å…¥åˆå§‹æç¤ºè¯")
                return
            
            # ä¿å­˜æµå¼è¾“å‡ºè®¾ç½®
            st.session_state.use_streaming = use_streaming
            
            # é…ç½®æ¨¡å‹
            models = {
                "optimizer": optimizer_model,
                "executor": executor_model,
                "evaluator": evaluator_model,
                "analyzer": analyzer_model
            }
            
            # ä¿å­˜é…ç½®åˆ°ä¼šè¯çŠ¶æ€
            st.session_state.task_description = task_description
            st.session_state.current_best_prompt = initial_prompt
            st.session_state.max_iterations = max_iterations
            st.session_state.auto_mode = auto_mode
            
            # é…ç½®API
            with st.spinner("æ­£åœ¨é…ç½®API..."):
                try:
                    result = configure_api(api_key, base_url, models)
                    if result:
                        st.success("âœ… APIé…ç½®æˆåŠŸ")
                        
                        # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
                        progress_text = "æ­£åœ¨ç”Ÿæˆæµ‹è¯•æ ·æœ¬..."
                        progress_bar = st.progress(0)
                        
                        # ç”Ÿæˆæµ‹è¯•æ ·æœ¬
                        with st.spinner(progress_text):
                            progress_bar.progress(25)
                            samples = generate_samples(task_description)
                            
                            if samples:
                                progress_bar.progress(50)
                                st.session_state.samples = samples
                                
                                # æ‰§è¡Œåˆå§‹æç¤º
                                progress_bar.progress(75)
                                outputs = run_current_best_prompt()
                                
                                if outputs:
                                    progress_bar.progress(100)
                                    time.sleep(0.5)  # ç»™ç”¨æˆ·ä¸€ç‚¹æ—¶é—´çœ‹åˆ°100%
                                    st.success("âœ… åˆå§‹åŒ–å®Œæˆï¼æ­£åœ¨è¿›å…¥ä¼˜åŒ–è¿‡ç¨‹...")
                                    st.session_state.initialized = True
                                    st.session_state.current_view = "optimization"
                                    st.session_state.is_optimizing = True
                                    time.sleep(1)  # ç»™ç”¨æˆ·ä¸€ç‚¹æ—¶é—´é˜…è¯»æˆåŠŸæ¶ˆæ¯
                                    st.rerun()
                            else:
                                progress_bar.progress(100)
                                st.error("âŒ ç”Ÿæˆæµ‹è¯•æ ·æœ¬å¤±è´¥")
                    else:
                        st.error("âŒ APIé…ç½®å¤±è´¥")
                except Exception as e:
                    st.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    # æ˜¾ç¤ºä¸€äº›ä½¿ç”¨æç¤º
    st.markdown("<h2 class='sub-header'>ä½¿ç”¨æŒ‡å—</h2>", unsafe_allow_html=True)
    st.markdown('<div class="card">', unsafe_allow_html=True)
    with st.expander("ğŸ“˜ SPO+æ˜¯ä»€ä¹ˆï¼Ÿ", expanded=False):
        st.markdown("""
        SPO+ (Self-Prompting Optimization Plus) æ˜¯ä¸€ä¸ªå¢å¼ºå‹è‡ªç›‘ç£æç¤ºä¼˜åŒ–ç³»ç»Ÿï¼Œå¯ä»¥è‡ªåŠ¨ä¼˜åŒ–æ‚¨çš„AIæç¤ºè¯ã€‚
        
        ç³»ç»Ÿé€šè¿‡ä»¥ä¸‹æ­¥éª¤å·¥ä½œï¼š
        1. åˆ†ææ‚¨çš„ä»»åŠ¡éœ€æ±‚å’Œåˆå§‹æç¤ºè¯
        2. è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•æ ·æœ¬
        3. æ‰§è¡Œä¼˜åŒ–è¿­ä»£ï¼Œç”Ÿæˆæ›´å¥½çš„æç¤ºè¯
        4. è¯„ä¼°å¹¶æ¯”è¾ƒä¸åŒæç¤ºè¯çš„æ•ˆæœ
        5. æä¾›è¯¦ç»†çš„åˆ†æå’Œä¼˜åŒ–ç»“æœ
        """)
    
    with st.expander("ğŸ” å¦‚ä½•è·å¾—æœ€ä½³æ•ˆæœï¼Ÿ", expanded=False):
        st.markdown("""
        ä¸ºäº†è·å¾—æœ€ä½³ä¼˜åŒ–æ•ˆæœï¼Œå»ºè®®æ‚¨ï¼š
        
        - **æä¾›è¯¦ç»†çš„ä»»åŠ¡æè¿°**ï¼šæ¸…æ™°è¯´æ˜æ‚¨å¸Œæœ›AIå®Œæˆçš„ä»»åŠ¡ã€ç›®æ ‡å—ä¼—å’ŒæœŸæœ›çš„è¾“å‡ºé£æ ¼
        - **æä¾›æœ‰è´¨é‡çš„åˆå§‹æç¤ºè¯**ï¼šåˆå§‹æç¤ºè¯è¶Šå¥½ï¼Œæœ€ç»ˆä¼˜åŒ–ç»“æœä¹Ÿä¼šè¶Šå¥½
        - **é€‚å½“è®¾ç½®è¿­ä»£æ¬¡æ•°**ï¼šç®€å•ä»»åŠ¡5-8æ¬¡è¿­ä»£è¶³å¤Ÿï¼Œå¤æ‚ä»»åŠ¡å¯èƒ½éœ€è¦10-15æ¬¡
        - **ä½¿ç”¨è‡ªåŠ¨æ¨¡å¼**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å®Œæˆæ•´ä¸ªä¼˜åŒ–è¿‡ç¨‹
        """)
        
    with st.expander("ğŸ”‘ å¦‚ä½•è·å–APIå¯†é’¥ï¼Ÿ", expanded=False):
        st.markdown("""
        æ‚¨éœ€è¦SiliconCloud APIå¯†é’¥æ‰èƒ½ä½¿ç”¨æœ¬ç³»ç»Ÿï¼š
        
        1. è®¿é—® [SiliconCloudå®˜ç½‘](https://cloud.siliconflow.cn/)
        2. æ³¨å†Œå¹¶ç™»å½•æ‚¨çš„è´¦æˆ·
        3. å¯¼èˆªåˆ°APIå¯†é’¥é¡µé¢åˆ›å»ºæ–°å¯†é’¥
        4. å¤åˆ¶ç”Ÿæˆçš„å¯†é’¥å¹¶ç²˜è´´åˆ°ä¸Šæ–¹çš„API Keyè¾“å…¥æ¡†
        """)
    st.markdown('</div>', unsafe_allow_html=True)

    # å¦‚æœåŠ è½½ç¤ºä¾‹
    if 'example_loaded' in st.session_state and st.session_state.example_loaded:
        st.session_state.example_loaded = False
        st.session_state._example_task = "åˆ›å»ºä¸€ä¸ªAIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿå›ç­”ç”¨æˆ·å…³äºå¥åº·å’Œè¥å…»çš„é—®é¢˜ï¼Œæä¾›ç§‘å­¦å‡†ç¡®ä¸”æ˜“äºç†è§£çš„ä¿¡æ¯ã€‚"
        st.session_state._example_prompt = "æˆ‘æ˜¯ä¸€ä¸ªå¥åº·é¡¾é—®AIåŠ©æ‰‹ã€‚è¯·å‘Šè¯‰æˆ‘ä½ çš„é—®é¢˜ï¼Œæˆ‘ä¼šå°½åŠ›æä¾›å¸®åŠ©ã€‚"
        # ä½¿ç”¨JavaScriptè‡ªåŠ¨å¡«å……è¡¨å•
        st.markdown(
            f"""
            <script>
                setTimeout(function() {{
                    document.querySelector('textarea[aria-label="ä»»åŠ¡éœ€æ±‚æè¿°"]').value = "{st.session_state._example_task}";
                    document.querySelector('textarea[aria-label="åˆå§‹æç¤ºè¯"]').value = "{st.session_state._example_prompt}";
                    
                    // è§¦å‘textAreaçš„inputäº‹ä»¶ä»¥æ›´æ–°Streamlitçš„çŠ¶æ€
                    const taskEvent = new Event('input', {{ bubbles: true }});
                    document.querySelector('textarea[aria-label="ä»»åŠ¡éœ€æ±‚æè¿°"]').dispatchEvent(taskEvent);
                    
                    const promptEvent = new Event('input', {{ bubbles: true }});
                    document.querySelector('textarea[aria-label="åˆå§‹æç¤ºè¯"]').dispatchEvent(promptEvent);
                }}, 500);
            </script>
            """,
            unsafe_allow_html=True
        )

# ä¼˜åŒ–è§†å›¾
def show_optimization_view():
    st.markdown("<h1 class='main-header'>SPO+ ä¼˜åŒ–è¿‡ç¨‹</h1>", unsafe_allow_html=True)
    
    # æ­¥éª¤æŒ‡ç¤ºå™¨
    st.markdown("""
    <div class="step-container">
        <div class="step completed">
            1
            <div class="step-label">APIé…ç½®</div>
        </div>
        <div class="step completed">
            2
            <div class="step-label">ä»»åŠ¡è®¾ç½®</div>
        </div>
        <div class="step active">
            3
            <div class="step-label">ä¼˜åŒ–è¿‡ç¨‹</div>
        </div>
        <div class="step">
            4
            <div class="step-label">ä¼˜åŒ–ç»“æœ</div>
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    # çŠ¶æ€æ 
    progress_value = st.session_state.current_iteration / st.session_state.max_iterations
    
    st.markdown('<div class="card">', unsafe_allow_html=True)
    col1, col2, col3 = st.columns([2, 6, 2])
    
    with col1:
        st.markdown(f"<h4>è¿­ä»£è¿›åº¦: {st.session_state.current_iteration}/{st.session_state.max_iterations}</h4>", unsafe_allow_html=True)
    
    with col2:
        progress = st.progress(progress_value)
    
    with col3:
        if st.session_state.is_optimizing:
            status = "ğŸ”„ æ­£åœ¨ä¼˜åŒ–..."
        else:
            status = "â¸ï¸ ç­‰å¾…æ“ä½œ..."
        st.markdown(f"<h4>çŠ¶æ€: {status}</h4>", unsafe_allow_html=True)
    st.markdown('</div>', unsafe_allow_html=True)
    
    # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¿­ä»£ï¼Œæ˜¾ç¤ºå¼•å¯¼æç¤º
    if st.session_state.current_iteration == 0:
        st.markdown("""
        <div class="guide-box">
            <h4>ğŸš€ ä¼˜åŒ–è¿‡ç¨‹å·²å¯åŠ¨</h4>
            <p>ç³»ç»Ÿæ­£åœ¨ä¼˜åŒ–æ‚¨çš„æç¤ºè¯ã€‚æ¯æ¬¡è¿­ä»£åŒ…æ‹¬ï¼šç”Ÿæˆæ–°æç¤ºè¯ â†’ æµ‹è¯•æ•ˆæœ â†’ è¯„ä¼°ç»“æœ â†’ å†³å®šæ˜¯å¦ä¿ç•™ã€‚æ‚¨å¯ä»¥åœ¨ä¸‹æ–¹æŸ¥çœ‹å®æ—¶è¿›å±•ã€‚</p>
        </div>
        """, unsafe_allow_html=True)
    
    # ä¸»è¦å†…å®¹
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.markdown("<h2 class='sub-header'>æç¤ºè¯æ¯”è¾ƒ</h2>", unsafe_allow_html=True)
        
        st.markdown('<div class="card">', unsafe_allow_html=True)
        st.markdown("**ğŸ“ å½“å‰æœ€ä½³æç¤º**")
        st.text_area("current_best_prompt", value=st.session_state.current_best_prompt, height=200, label_visibility="collapsed")
        
        if st.session_state.new_prompt:
            st.markdown("**âœ¨ æ–°å€™é€‰æç¤º**")
            st.text_area("new_prompt", value=st.session_state.new_prompt, height=200, label_visibility="collapsed")
        st.markdown('</div>', unsafe_allow_html=True)
        
        if st.session_state.analysis:
            st.markdown("<h2 class='sub-header'>æ”¹è¿›åˆ†æ</h2>", unsafe_allow_html=True)
            st.markdown('<div class="card">', unsafe_allow_html=True)
            st.markdown(st.session_state.analysis)
            st.markdown('</div>', unsafe_allow_html=True)
        
        # æ§åˆ¶æŒ‰é’®
        if not st.session_state.is_optimizing:
            st.markdown('<div class="card">', unsafe_allow_html=True)
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("â–¶ï¸ ç»§ç»­ä¼˜åŒ–", key="continue_btn", use_container_width=True):
                    st.session_state.is_optimizing = True
                    st.rerun()
            
            with col2:
                if st.button("âœ… å®Œæˆä¼˜åŒ–", key="finish_btn", use_container_width=True):
                    st.session_state.current_view = "results"
                    st.rerun()
            st.markdown('</div>', unsafe_allow_html=True)
    
    with col2:
        st.markdown("<h2 class='sub-header'>æµ‹è¯•æ ·æœ¬</h2>", unsafe_allow_html=True)
        
        # æ˜¾ç¤ºæ ·æœ¬å’Œè¾“å‡º
        for sample in st.session_state.samples:
            sample_id = sample['id']
            
            with st.expander(f"æ ·æœ¬ {sample_id}: {sample['question'][:50]}...", expanded=(sample_id == 1)):
                st.markdown(f"**é—®é¢˜:**")
                st.markdown(f'<div class="output-container">{sample["question"]}</div>', unsafe_allow_html=True)
                
                if sample_id in st.session_state.current_best_outputs:
                    st.markdown("**å½“å‰è¾“å‡º:**")
                    current_output = st.session_state.current_best_outputs[sample_id]
                    st.markdown(f"<div class='output-container'>{current_output}</div>", unsafe_allow_html=True)
                
                if st.session_state.new_outputs and sample_id in st.session_state.new_outputs:
                    st.markdown("**æ–°è¾“å‡º:**")
                    new_output = st.session_state.new_outputs[sample_id]
                    
                    # æ·»åŠ è¯„ä¼°ç»“æœæ ·å¼
                    css_class = ""
                    result_icon = ""
                    if st.session_state.evaluations and sample_id in st.session_state.evaluations:
                        result = st.session_state.evaluations[sample_id]
                        if result == "Bæ›´å¥½":
                            css_class = "better"
                            result_icon = "âœ… "
                        elif result == "Aæ›´å¥½":
                            css_class = "worse"
                            result_icon = "âŒ "
                        else:
                            css_class = "similar"
                            result_icon = "âš–ï¸ "
                    
                    st.markdown(f"<div class='output-container {css_class}'>{new_output}</div>", unsafe_allow_html=True)
                    
                    if st.session_state.evaluations and sample_id in st.session_state.evaluations:
                        result = st.session_state.evaluations[sample_id]
                        st.markdown(f"**è¯„ä¼°ç»“æœ:** {result_icon}{result}")
        
        # ä¼˜åŒ–å†å²
        if st.session_state.optimization_history:
            st.markdown("<h2 class='sub-header'>ä¼˜åŒ–å†å²</h2>", unsafe_allow_html=True)
            
            st.markdown('<div class="card">', unsafe_allow_html=True)
            for item in st.session_state.optimization_history:
                css_class = "better" if item["is_better"] else "not-better"
                icon = "âœ…" if item["is_better"] else "âš ï¸"
                
                with st.expander(f"{icon} è¿­ä»£ {item['iteration']} - {'æ”¹è¿›æˆåŠŸ' if item['is_better'] else 'æœªæ”¹è¿›'}", expanded=False):
                    st.markdown(f"**æç¤ºè¯:**")
                    st.text_area(f"prompt_{item['iteration']}", value=item['prompt'], height=100, label_visibility="collapsed")
                    
                    st.markdown("**è¯„ä¼°ç»“æœ:**")
                    for sample_id, result in item['evaluations'].items():
                        if result == "Bæ›´å¥½":
                            result_emoji = "âœ…"
                        elif result == "Aæ›´å¥½":
                            result_emoji = "âŒ"
                        else:
                            result_emoji = "âš–ï¸"
                        st.markdown(f"- æ ·æœ¬ {sample_id}: {result_emoji} {result}")
                    
                    with st.expander("ğŸ“Š è¯¦ç»†åˆ†æ", expanded=False):
                        st.markdown(item['analysis'])
            st.markdown('</div>', unsafe_allow_html=True)
    
    # å¦‚æœæ­£åœ¨ä¼˜åŒ–ä¸”éœ€è¦æ‰§è¡Œä¸‹ä¸€æ­¥
    if st.session_state.is_optimizing:
        run_optimization_step_with_ui()

# å¸¦UIåé¦ˆçš„ä¼˜åŒ–æ­¥éª¤æ‰§è¡Œ
def run_optimization_step_with_ui():
    if st.session_state.current_iteration >= st.session_state.max_iterations:
        st.session_state.is_optimizing = False
        st.session_state.current_view = "results"
        st.rerun()
        return
    
    st.session_state.current_iteration += 1
    
    # åˆ›å»ºå®¹å™¨æ¥æ˜¾ç¤ºè¿›åº¦
    status_container = st.empty()
    output_container = st.empty()
    
    status_container.markdown(f"### ğŸ”„ æ‰§è¡Œç¬¬ {st.session_state.current_iteration} æ¬¡ä¼˜åŒ–...")
    
    # 1. ç”Ÿæˆæ–°æç¤ºå€™é€‰
    output_container.markdown("ğŸ§  æ­£åœ¨ç”Ÿæˆä¼˜åŒ–åçš„æç¤ºè¯...")
    new_prompt = optimize_prompt(
        st.session_state.current_best_prompt,
        json.dumps(st.session_state.current_best_outputs),
        st.session_state.task_description,
        get_optimization_history_summary()
    )
    
    if not new_prompt:
        output_container.error("âŒ ä¼˜åŒ–æç¤ºè¯å¤±è´¥")
        st.session_state.is_optimizing = False
        return
    
    st.session_state.new_prompt = new_prompt
    output_container.markdown("âœ… æç¤ºè¯ç”Ÿæˆå®Œæˆ")
    
    # 2. æ‰§è¡Œæ–°æç¤º
    output_container.markdown("ğŸ” æ­£åœ¨æµ‹è¯•æ–°æç¤ºè¯...")
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
    use_stream = st.session_state.get('use_streaming', False)
    
    new_outputs = {}
    # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ›å»ºä¸€ä¸ªè¿›åº¦æ¡
    progress_bars = {}
    output_blocks = {}
    
    for sample in st.session_state.samples:
        sample_id = sample['id']
        progress_bars[sample_id] = output_container.progress(0)
        output_blocks[sample_id] = output_container.empty()
        
        # æ›´æ–°çŠ¶æ€
        status_container.markdown(f"### ğŸ”„ æµ‹è¯•æ ·æœ¬ {sample_id}/{len(st.session_state.samples)}...")
        
        if use_stream:
            # ä½¿ç”¨æµå¼è¾“å‡º
            output_blocks[sample_id].markdown(f"ç”Ÿæˆæ ·æœ¬ {sample_id} çš„å›ç­”:")
            output = execute_prompt(new_prompt, sample['question'], use_stream=True)
        else:
            # å¸¸è§„è¾“å‡º
            output = execute_prompt(new_prompt, sample['question'])
            # æ¨¡æ‹Ÿè¿›åº¦
            for i in range(10):
                progress_bars[sample_id].progress((i+1)/10)
                time.sleep(0.1)
            output_blocks[sample_id].markdown(f"æ ·æœ¬ {sample_id} å›ç­”å·²å®Œæˆ")
            
        new_outputs[sample_id] = output
        progress_bars[sample_id].progress(1.0)
    
    # æ¸…ç†è¿›åº¦æ¡å’Œå—
    for sample_id in progress_bars:
        progress_bars[sample_id].empty()
        output_blocks[sample_id].empty()
        
    st.session_state.new_outputs = new_outputs
    output_container.markdown("âœ… æµ‹è¯•å®Œæˆ")
    
    # 3. è¯„ä¼°æ–°æ—§è¾“å‡º
    output_container.markdown("âš–ï¸ æ­£åœ¨è¯„ä¼°è¾“å‡ºè´¨é‡...")
    evaluations = {}
    
    eval_progress = output_container.progress(0)
    for idx, sample in enumerate(st.session_state.samples):
        sample_id = sample['id']
        output_a = st.session_state.current_best_outputs.get(sample_id, "")
        output_b = new_outputs.get(sample_id, "")
        
        evaluation = evaluate_outputs(
            output_a,
            output_b,
            st.session_state.task_description,
            sample['question']
        )
        
        evaluations[sample_id] = evaluation
        eval_progress.progress((idx + 1) / len(st.session_state.samples))
    
    eval_progress.empty()
    st.session_state.evaluations = evaluations
    output_container.markdown("âœ… è¯„ä¼°å®Œæˆ")
    
    # 4. åˆ†ææç¤ºå˜åŒ–
    output_container.markdown("ğŸ” æ­£åœ¨åˆ†ææç¤ºè¯å˜åŒ–...")
    analysis = analyze_changes(
        st.session_state.current_best_prompt,
        new_prompt,
        st.session_state.task_description
    )
    
    st.session_state.analysis = analysis
    output_container.markdown("âœ… åˆ†æå®Œæˆ")
    
    # 5. æ ¹æ®è¯„ä¼°ç»“æœæ›´æ–°æœ€ä½³æç¤º
    is_better = should_update_best_prompt(evaluations)
    if is_better:
        st.session_state.current_best_prompt = new_prompt
        st.session_state.current_best_outputs = new_outputs
        output_container.markdown("ğŸ‰ å‘ç°æ›´å¥½çš„æç¤ºè¯ï¼å·²æ›´æ–°ä¸ºå½“å‰æœ€ä½³æç¤ºã€‚")
    else:
        output_container.markdown("ğŸ“Œ æ–°æç¤ºè¯æœªèƒ½æä¾›æ”¹è¿›ï¼Œä¿æŒå½“å‰æœ€ä½³æç¤ºä¸å˜ã€‚")
    
    # è®°å½•ä¼˜åŒ–å†å²
    st.session_state.optimization_history.append({
        "iteration": st.session_state.current_iteration,
        "prompt": new_prompt,
        "is_better": is_better,
        "analysis": analysis,
        "evaluations": evaluations
    })
    
    # æ¸…ç©ºçŠ¶æ€å®¹å™¨å’Œè¾“å‡ºå®¹å™¨
    status_container.empty()
    output_container.empty()
    
    # å¦‚æœæ˜¯è‡ªåŠ¨æ¨¡å¼ï¼Œç»§ç»­ä¼˜åŒ–
    if st.session_state.auto_mode:
        time.sleep(1)  # çŸ­æš‚å»¶è¿Ÿï¼Œè®©UIæ›´æ–°
        st.rerun()  # é‡æ–°åŠ è½½é¡µé¢ç»§ç»­ä¼˜åŒ–
    else:
        st.session_state.is_optimizing = False
        st.rerun()  # é‡æ–°åŠ è½½é¡µé¢ç­‰å¾…ç”¨æˆ·æ“ä½œ

# ç»“æœè§†å›¾
def show_results_view():
    st.markdown("<h1 class='main-header'>SPO+ ä¼˜åŒ–ç»“æœ</h1>", unsafe_allow_html=True)
    
    # æ­¥éª¤æŒ‡ç¤ºå™¨
    st.markdown("""
    <div class="step-container">
        <div class="step completed">
            1
            <div class="step-label">APIé…ç½®</div>
        </div>
        <div class="step completed">
            2
            <div class="step-label">ä»»åŠ¡è®¾ç½®</div>
        </div>
        <div class="step completed">
            3
            <div class="step-label">ä¼˜åŒ–è¿‡ç¨‹</div>
        </div>
        <div class="step active">
            4
            <div class="step-label">ä¼˜åŒ–ç»“æœ</div>
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    # ç»“æœæ‘˜è¦
    st.markdown("<h2 class='sub-header'>ä¼˜åŒ–æ‘˜è¦</h2>", unsafe_allow_html=True)
    
    st.markdown('<div class="card">', unsafe_allow_html=True)
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("æ€»è¿­ä»£æ¬¡æ•°", st.session_state.current_iteration)
    
    with col2:
        better_count = sum(1 for item in st.session_state.optimization_history if item["is_better"])
        st.metric("æˆåŠŸæ”¹è¿›æ¬¡æ•°", better_count)
    
    with col3:
        improvement_rate = better_count / st.session_state.current_iteration if st.session_state.current_iteration > 0 else 0
        st.metric("æ”¹è¿›æˆåŠŸç‡", f"{improvement_rate:.0%}")
    
    # æç¤ºæ¶ˆæ¯
    if improvement_rate > 0.5:
        st.success("ğŸ‰ ä¼˜åŒ–æ•ˆæœè‰¯å¥½ï¼è¶…è¿‡åŠæ•°çš„è¿­ä»£æä¾›äº†æ”¹è¿›ã€‚")
    elif improvement_rate > 0.2:
        st.info("ğŸ‘ ä¼˜åŒ–å–å¾—äº†ä¸€å®šæˆæ•ˆï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´ã€‚")
    else:
        st.warning("âš ï¸ ä¼˜åŒ–è¿‡ç¨‹è¾ƒä¸ºå›°éš¾ï¼Œå¯èƒ½éœ€è¦æ›´å¥½çš„åˆå§‹æç¤ºè¯æˆ–æ›´ç²¾ç¡®çš„ä»»åŠ¡æè¿°ã€‚")
    st.markdown('</div>', unsafe_allow_html=True)
    
    # æœ€ç»ˆæç¤ºè¯
    st.markdown("<h2 class='sub-header'>æœ€ç»ˆä¼˜åŒ–æç¤ºè¯</h2>", unsafe_allow_html=True)
    
    st.markdown('<div class="card">', unsafe_allow_html=True)
    final_prompt = st.text_area("final_prompt", value=st.session_state.current_best_prompt, height=300, label_visibility="collapsed")
    
    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown("""
        <div class="tip-box">
            <p>ğŸ’¡ æç¤ºï¼šæ‚¨å¯ä»¥å°†æ­¤æç¤ºè¯ç”¨äºå®é™…åº”ç”¨ã€‚æ”¹è¿›åçš„æç¤ºè¯åº”è¯¥èƒ½å¤Ÿç”Ÿæˆæ›´é«˜è´¨é‡çš„AIå›å¤ã€‚</p>
        </div>
        """, unsafe_allow_html=True)
    with col2:
        copy_button = st.button("ğŸ“‹ å¤åˆ¶æç¤ºè¯", use_container_width=True)
        
    if copy_button:
        st.code(st.session_state.current_best_prompt, language="markdown")
        st.success("âœ… æç¤ºè¯å·²ç”Ÿæˆä»£ç å—ï¼Œå¯ç›´æ¥å¤åˆ¶ï¼")
    st.markdown('</div>', unsafe_allow_html=True)
    
    # ä¼˜åŒ–æ•ˆæœæµ‹è¯•
    st.markdown("<h2 class='sub-header'>ä¼˜åŒ–æ•ˆæœæµ‹è¯•</h2>", unsafe_allow_html=True)
    
    st.markdown('<div class="card">', unsafe_allow_html=True)
    st.markdown("""
    <div class="guide-box">
        <h4>ğŸ§ª æµ‹è¯•ä¼˜åŒ–åçš„æç¤ºè¯</h4>
        <p>è¾“å…¥ä¸€ä¸ªé—®é¢˜ï¼Œçœ‹çœ‹ä¼˜åŒ–åçš„æç¤ºè¯å¦‚ä½•å›ç­”ã€‚æ‚¨å¯ä»¥æ‰“å¼€æµå¼è¾“å‡ºæŸ¥çœ‹ç”Ÿæˆè¿‡ç¨‹ã€‚</p>
    </div>
    """, unsafe_allow_html=True)
    
    user_question = st.text_area("è¾“å…¥æµ‹è¯•é—®é¢˜", placeholder="è¯·è¾“å…¥ä¸€ä¸ªé—®é¢˜æ¥æµ‹è¯•ä¼˜åŒ–åçš„æç¤ºè¯æ•ˆæœ...", height=100)
    use_stream_test = st.checkbox("ä½¿ç”¨æµå¼è¾“å‡º", value=True)
    
    if st.button("ğŸš€ æµ‹è¯•æç¤ºè¯", use_container_width=True):
        if user_question:
            with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
                response = execute_prompt(
                    st.session_state.current_best_prompt, 
                    user_question,
                    use_stream=use_stream_test
                )
            if not use_stream_test:
                st.markdown("<div class='output-container better'>", unsafe_allow_html=True)
                st.markdown(response)
                st.markdown("</div>", unsafe_allow_html=True)
        else:
            st.warning("âš ï¸ è¯·è¾“å…¥æµ‹è¯•é—®é¢˜")
    st.markdown('</div>', unsafe_allow_html=True)
    
    # ä¼˜åŒ–å†å²
    if st.session_state.optimization_history:
        st.markdown("<h2 class='sub-header'>ä¼˜åŒ–å†å²</h2>", unsafe_allow_html=True)
        
        st.markdown('<div class="card">', unsafe_allow_html=True)
        # å›¾è¡¨æ˜¾ç¤ºä¼˜åŒ–è¿›åº¦
        iteration_data = [(i+1) for i in range(len(st.session_state.optimization_history))]
        better_data = [1 if item["is_better"] else 0 for item in st.session_state.optimization_history]
        
        import pandas as pd
        import altair as alt
        
        df = pd.DataFrame({
            'è¿­ä»£': iteration_data,
            'æ˜¯å¦æ”¹è¿›': better_data
        })
        
        base = alt.Chart(df).encode(
            x=alt.X('è¿­ä»£:O', axis=alt.Axis(title='è¿­ä»£æ¬¡æ•°')),
            y=alt.Y('æ˜¯å¦æ”¹è¿›:Q', axis=alt.Axis(title='æ”¹è¿›çŠ¶æ€'))
        )
        
        bars = base.mark_bar(color='#3B82F6').encode(
            color=alt.condition(
                alt.datum['æ˜¯å¦æ”¹è¿›'] == 1,
                alt.value('#10B981'),  # æˆåŠŸæ”¹è¿›
                alt.value('#EF4444')   # æœªæ”¹è¿›
            )
        )
        
        st.altair_chart(bars, use_container_width=True)
        
        # ä¼˜åŒ–å†å²è¯¦æƒ…
        for item in st.session_state.optimization_history:
            css_class = "better" if item["is_better"] else "not-better"
            icon = "âœ…" if item["is_better"] else "âš ï¸"
            
            with st.expander(f"{icon} è¿­ä»£ {item['iteration']} - {'æ”¹è¿›æˆåŠŸ' if item['is_better'] else 'æœªæ”¹è¿›'}", expanded=False):
                st.markdown(f"**æç¤ºè¯:**")
                st.text_area(f"result_prompt_{item['iteration']}", value=item['prompt'], height=100, label_visibility="collapsed")
                
                better_count = sum(1 for result in item['evaluations'].values() if result == "Bæ›´å¥½")
                worse_count = sum(1 for result in item['evaluations'].values() if result == "Aæ›´å¥½")
                similar_count = sum(1 for result in item['evaluations'].values() if result == "ç›¸ä¼¼")
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("æ›´å¥½ç»“æœ", better_count)
                with col2:
                    st.metric("æ›´å·®ç»“æœ", worse_count)
                with col3:
                    st.metric("ç›¸ä¼¼ç»“æœ", similar_count)
                
                st.markdown("**è¯„ä¼°ç»“æœè¯¦æƒ…:**")
                for sample_id, result in item['evaluations'].items():
                    if result == "Bæ›´å¥½":
                        result_emoji = "âœ…"
                        result_color = "#10B981"
                    elif result == "Aæ›´å¥½":
                        result_emoji = "âŒ"
                        result_color = "#EF4444"
                    else:
                        result_emoji = "âš–ï¸"
                        result_color = "#F59E0B"
                    st.markdown(f"<span style='color:{result_color}'>- æ ·æœ¬ {sample_id}: {result_emoji} {result}</span>", unsafe_allow_html=True)
                
                with st.expander("ğŸ“Š è¯¦ç»†åˆ†æ", expanded=False):
                    st.markdown(item['analysis'])
        st.markdown('</div>', unsafe_allow_html=True)
    
    # å¯¼å‡ºå’Œé‡ç½®æŒ‰é’®
    st.markdown('<div class="card">', unsafe_allow_html=True)
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ“¥ å¯¼å‡ºå†å²", use_container_width=True):
            export_data = {
                "date": time.strftime("%Y-%m-%d %H:%M:%S"),
                "task_description": st.session_state.task_description,
                "initial_prompt": st.session_state.current_best_prompt,
                "iterations": st.session_state.current_iteration,
                "history": [{
                    "iteration": item["iteration"],
                    "prompt": item["prompt"],
                    "is_better": item["is_better"],
                    "analysis": item["analysis"]
                } for item in st.session_state.optimization_history]
            }
            
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½JSONæ–‡ä»¶",
                data=json.dumps(export_data, ensure_ascii=False, indent=2),
                file_name=f"spo-plus-history-{time.strftime('%Y%m%d-%H%M%S')}.json",
                mime="application/json",
                use_container_width=True
            )
    
    with col2:
        if st.button("ğŸ”„ å¼€å§‹æ–°çš„ä¼˜åŒ–", use_container_width=True):
            # ä¿å­˜APIè®¾ç½®ä»¥ä¾¿é‡ç”¨
            api_key = st.session_state.get('api_key', '')
            base_url = st.session_state.get('base_url', API_BASE_URL)
            
            # é‡ç½®ä¼šè¯çŠ¶æ€
            for key in list(st.session_state.keys()):
                if key != "available_models":
                    del st.session_state[key]
            
            # æ¢å¤APIè®¾ç½®
            st.session_state.api_key = api_key
            st.session_state.base_url = base_url
            
            # é‡ç½®çŠ¶æ€
            st.session_state.initialized = False
            st.session_state.api_configured = api_key != ''
            st.session_state.current_view = "config"
            st.session_state.current_iteration = 0
            st.session_state.max_iterations = 10
            st.session_state.samples = []
            st.session_state.current_best_prompt = ""
            st.session_state.current_best_outputs = {}
            st.session_state.new_prompt = ""
            st.session_state.new_outputs = {}
            st.session_state.evaluations = {}
            st.session_state.analysis = ""
            st.session_state.optimization_history = []
            st.session_state.is_optimizing = False
            
            st.success("âœ… å·²é‡ç½®ï¼æ‚¨å¯ä»¥å¼€å§‹æ–°çš„ä¼˜åŒ–ä»»åŠ¡ã€‚")
            st.rerun()
    st.markdown('</div>', unsafe_allow_html=True)

# ä¸»åº”ç”¨
def main():
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.markdown("")
        st.markdown("å¢å¼ºå‹è‡ªç›‘ç£æç¤ºä¼˜åŒ–ç³»ç»Ÿ")
        
        st.markdown("---")
        
        if st.button("é…ç½®"):
            st.session_state.current_view = "config"
            st.rerun()
        
        if st.session_state.initialized:
            if st.button("ä¼˜åŒ–è¿‡ç¨‹"):
                st.session_state.current_view = "optimization"
                st.rerun()
            
            if st.button("ä¼˜åŒ–ç»“æœ"):
                st.session_state.current_view = "results"
                st.rerun()
        
        st.markdown("---")
        st.markdown("### å…³äº")
        st.markdown("""
        SPO+æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æç¤ºè¯ä¼˜åŒ–ç³»ç»Ÿï¼Œç»“åˆäº†è‡ªåŠ¨åŒ–ä¼˜åŒ–å’Œäººç±»åé¦ˆï¼Œå¸®åŠ©ç”¨æˆ·åˆ›å»ºæ›´é«˜è´¨é‡çš„æç¤ºè¯ã€‚
        
        **ç‰¹ç‚¹:**
        - è‡ªåŠ¨ä¼˜åŒ–
        - æµå¼è¾“å‡º
        - äººæœºåä½œ
        - å¯è§†åŒ–ç•Œé¢
        """)
    
    # ä¸»å†…å®¹
    if st.session_state.current_view == "config":
        show_config_view()
    elif st.session_state.current_view == "optimization":
        show_optimization_view()
    elif st.session_state.current_view == "results":
        show_results_view()

if __name__ == "__main__":
    main() 
